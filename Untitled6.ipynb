{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import string\n",
    "import mmap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "import spacy as sp\n",
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://tools.wmflabs.org/wsexport/tool/book.php?lang=it&format=txt&page=Dichiarazione_Universale_dei_Diritti_dell%2527Uomo_-_UNGA%2C_10_dicembre_1948'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.121 Safari/537.36'}\n",
    "result = requests.get(url, allow_redirects=True, headers=headers)\n",
    "testo = result.text\n",
    "file_name = \"dichiarazione_universale_dei_diritti_dell_uomo.txt\"\n",
    "with open(file_name, 'wb') as t:\n",
    "    t.write(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open txt file and filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_end_parse(counter,start,end):\n",
    "    if counter > start and counter < end:\n",
    "        return True\n",
    "    else:\n",
    "        return False \n",
    "\n",
    "lista_frasi = []\n",
    "with open(file_name, \"r+b\") as f:\n",
    "    mm = mmap.mmap(f.fileno(), 0)\n",
    "    row = 'XXXX'\n",
    "    cnt = 0\n",
    "    while len(row) > 0:\n",
    "        row = mm.readline().decode('utf-8')\n",
    "        if not re.match(\"\\s+\",row) and start_end_parse(cnt,29,360) and not re.match(\"^Articolo\\W\\d+$\",row):\n",
    "            lista_frasi.append(row.strip())  \n",
    "        cnt+=1\n",
    "        \n",
    "testo_new = ' '.join(lista_frasi) \n",
    "testo_new_5_par = ' '.join(lista_frasi[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizzazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_it = sp.load(\"it_core_news_sm\")\n",
    "doc = nlp_it(testo_new)\n",
    "doc_5_par = nlp_it(testo_new_5_par)\n",
    "tokens = [tok.text for tok in doc if not tok.is_stop and not tok.is_punct]\n",
    "tokens_no_prep = [tok for tok in tokens if len(tok)>1 and tok.isalpha()]\n",
    "#tokens_lemma = [tok.lemma_ for tok in doc if not tok.is_stop and not tok.is_punct and len(tok.text)>1 and tok.text.isalpha()] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Estrazione dei parametri numerici più significativi del testo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La lunghezza del testo è 11530\n",
      "La lunghezza media di una frase 27.14\n",
      "Il numero di parole utilizzate sono 959\n",
      "Il numero di parole uniche utilizzate sono 469\n",
      "Il numero di frasi utilizzate sono 72\n"
     ]
    }
   ],
   "source": [
    "print(\"La lunghezza del testo è {}\".format(len(testo_new)))\n",
    "print(\"La lunghezza media di una frase {:.2f}\".format(np.mean([len(fr) for fr in list(doc.sents)])))\n",
    "print(\"Il numero di parole utilizzate sono {}\".format(len(tokens)))\n",
    "print(\"Il numero di parole uniche utilizzate sono {}\".format(len(set(tokens))))\n",
    "print(\"Il numero di frasi utilizzate sono {}\".format(len(list(doc.sents))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Estrazione del vocabolario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parola          Occorrenze\n",
      "---------------------------\n",
      "e               85\n",
      "diritto         40\n",
      "individuo       38\n",
      "o               27\n",
      "diritti         21\n",
      "libertà         21\n",
      "l'              13\n",
      "i               12\n",
      "potrà           9\n",
      "umani           8\n",
      "è               8\n",
      "Nazioni         8\n",
      "Considerato     7\n",
      "istruzione      7\n",
      "famiglia        6\n",
      "dell'           6\n",
      "sociale         6\n",
      "dignità         5\n",
      "Unite           5\n",
      "fondamentali    5\n",
      "\n",
      "Parola          Occorrenze\n",
      "---------------------------\n",
      "diritto         40\n",
      "individuo       38\n",
      "diritti         21\n",
      "libertà         21\n",
      "potrà           9\n",
      "umani           8\n",
      "Nazioni         8\n",
      "Considerato     7\n",
      "istruzione      7\n",
      "famiglia        6\n",
      "sociale         6\n",
      "dignità         5\n",
      "Unite           5\n",
      "fondamentali    5\n",
      "rispetto        5\n",
      "presente        5\n",
      "società         5\n",
      "Dichiarazione   5\n",
      "internazionale  5\n",
      "religione       5\n"
     ]
    }
   ],
   "source": [
    "voc_base = Counter(tokens)\n",
    "voc_base2 = Counter(tokens_no_prep) # vocabolario senza preposizioni\n",
    "#voc_base3 = Counter(tokens_lemma) # vocabolario dei lemmi senza preposizioni\n",
    "\n",
    "display_voc(voc_base,20,15)\n",
    "display_voc(voc_base2,20,15)\n",
    "#display_voc(voc_base3,20,15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part of Speech Tagging intero testo\n",
    "L=[]\n",
    "for token in doc:\n",
    "    L.append([token, \n",
    "              token.lemma_, \n",
    "              token.pos_, \n",
    "              token.tag_,\n",
    "              #token.is_alpha, \n",
    "              #token.is_punct, \n",
    "              #token.is_space,\n",
    "              #token.is_stop\n",
    "             ])\n",
    "x = pd.DataFrame(L,columns=[\"word\",\n",
    "                            \"lemma\",\n",
    "                            \"pos\",\n",
    "                            \"tag\",\n",
    "                            #\"is_alpha\",\"is_punct\",\"is_space\",\"is_stop\"\n",
    "                           ])\n",
    "#x.to_csv(\"part_of_speech_tagging.csv\",index = False)\n",
    "\n",
    "#Part of Speech Tagging dei primi 5 paragrafi\n",
    "L5=[]\n",
    "for token in doc_5_par:\n",
    "    L5.append([token, \n",
    "              token.lemma_, \n",
    "              token.pos_, \n",
    "              token.tag_,\n",
    "              #token.is_alpha, \n",
    "              #token.is_punct, \n",
    "              #token.is_space,\n",
    "              #token.is_stop\n",
    "             ])\n",
    "x5 = pd.DataFrame(L5,columns=[\"word\",\n",
    "                            \"lemma\",\n",
    "                            \"pos\",\n",
    "                            \"tag\",\n",
    "                            #\"is_alpha\",\"is_punct\",\"is_space\",\"is_stop\"\n",
    "                           ])\n",
    "x5.to_csv(\"part_of_speech_tagging_5_par.csv\",index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Estrazione entità nominali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parola               Occorrenze\n",
      "--------------------------------\n",
      "Nazioni Unite        5\n",
      "Dichiarazione        5\n",
      "Stato                4\n",
      "Nazioni              3\n",
      "Stati membri         2\n",
      "Statuto              1\n",
      "ASSEMBLEA GENERALE   1\n",
      "Uomini               1\n"
     ]
    }
   ],
   "source": [
    "entita = [ent.text for ent in doc.ents]\n",
    "voc_entita = Counter(entita)\n",
    "display_voc(voc_entita,20,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Estrazione forme multi-word di significato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parola                         Occorrenze\n",
      "------------------------------------------\n",
      "diritti umani                  6\n",
      "diritto alla libertà           4\n",
      "esseri umani                   2\n",
      "tenore di vita                 2\n",
      "Stati membri                   2\n",
      "libertà fondamentali           2\n",
      "diritto alla sicurezza         2\n",
      "riconoscimento della dignità   1\n",
      "membri della famiglia          1\n",
      "famiglia umana                 1\n",
      "fondamento della libertà       1\n",
      "atti di barbarie               1\n",
      "libertà di parola              1\n",
      "norme giuridiche               1\n",
      "sviluppo di rapporti           1\n",
      "rapporti amichevoli            1\n",
      "valore della persona           1\n",
      "persona umana                  1\n",
      "organo della società           1\n",
      "misure progressive             1\n"
     ]
    }
   ],
   "source": [
    "my_matcher = Matcher(doc.vocab)\n",
    "pattern_na = [{\"POS\": \"NOUN\"}, {\"POS\": \"ADJ\"}]\n",
    "my_matcher.add(\"NOME_AGGETTIVO\", None, pattern_na)\n",
    "pattern_nn = [{\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\"}]\n",
    "my_matcher.add(\"NOME_NOME\", None, pattern_nn)\n",
    "pattern_nan = [{\"POS\": \"NOUN\"}, {\"POS\": \"ADP\"}, {\"POS\": \"NOUN\"}]\n",
    "my_matcher.add(\"NOME_ADP_NOME\", None, pattern_nan)\n",
    "\n",
    "matches = my_matcher(doc)\n",
    "out = []\n",
    "for mathc_id, start, end in matches:\n",
    "        span = doc[start:end]\n",
    "        out.append(span.text)\n",
    "\n",
    "voc_multi_words = Counter(out)\n",
    "display_voc(voc_multi_words,20,30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
